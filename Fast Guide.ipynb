{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsdfyd/.venv/venv34/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from reskit.norms import binar_norm, wbysqdist\n",
    "from reskit.norms import spectral_norm\n",
    "\n",
    "from reskit.features.degree import degrees \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from reskit.core import Transformer, Pipeliner\n",
    "\n",
    "def orig(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция чтения данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_autism(path_to_read='Data/dti/', distances=True):\n",
    "    def get_autism_distances(loc_name):\n",
    "        with open(loc_name, 'r') as f:\n",
    "            read_data = f.readlines()\n",
    "\n",
    "        read_data = pd.DataFrame(\n",
    "            np.array([np.array(item[:-1].split()).astype(int) for item in read_data]))\n",
    "\n",
    "        return read_data\n",
    "\n",
    "    def get_distance_matrix(coords):\n",
    "        if type(coords) == pd.core.frame.DataFrame:\n",
    "            coords = coords.values\n",
    "        elif type(coords) != np.ndarray:\n",
    "            print('Provide either pandas df or numpy array!')\n",
    "            return -1\n",
    "\n",
    "        shape = len(coords)\n",
    "        dist_matrix = np.zeros((shape, shape))\n",
    "        del shape\n",
    "        for i in range(len(coords)):\n",
    "            for j in range(i + 1, len(coords)):\n",
    "                dist_matrix[i, j] = np.linalg.norm(coords[i, :] - coords[j, :])\n",
    "                dist_matrix[j, i] = dist_matrix[i, j]\n",
    "        return dist_matrix\n",
    "\n",
    "    target_vector = []  # this will be a target vector (diagnosis)\n",
    "    matrices = []  # this will be a list of connectomes\n",
    "    all_files = sorted(os.listdir(path_to_read))\n",
    "    matrix_files = [\n",
    "        item for item in all_files if 'DTI_connectivity' in item and 'All' not in item]\n",
    "    distance_files = [\n",
    "        item for item in all_files if 'DTI_region_xyz_centers' in item and 'All' not in item]\n",
    "\n",
    "    # for each file in a sorted (!) list of files:\n",
    "    for filename in matrix_files:\n",
    "\n",
    "        A_dataframe = pd.read_csv(\n",
    "            path_to_read + filename, sep='   ', header=None, engine='python')\n",
    "        A = A_dataframe.values  # we will use a list of numpy arrays, NOT pandas dataframes\n",
    "        matrices.append(A)  # append a matrix to our list\n",
    "        if \"ASD\" in filename:\n",
    "            target_vector.append(1)\n",
    "        elif \"TD\" in filename:\n",
    "            target_vector.append(0)\n",
    "    asd_dict = {}\n",
    "    asd_dict['X'] = np.array(matrices)\n",
    "    asd_dict['y'] = np.array(target_vector)\n",
    "    if distances:\n",
    "        dist_matrix_list = []\n",
    "        for item in distance_files:\n",
    "            # print(item)\n",
    "            cur_coord = get_autism_distances(path_to_read + item)\n",
    "            cur_dist_mtx = get_distance_matrix(cur_coord)\n",
    "            dist_matrix_list += [cur_dist_mtx]\n",
    "\n",
    "        asd_dict['dist'] = np.array(dist_matrix_list)\n",
    "\n",
    "    return asd_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объекты кросс валидации для поиска наилучших параметров грид серчем и для валидирования получившейся модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=0)\n",
    "\n",
    "eval_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если понадобится выполнить только один пайплайн, то можно сделать следующее. Допустим мы хотим применить к данным бинарную нормировку и получить степени вершин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = 'Data/dti/'\n",
    "data = Transformer(get_autism).fit_transform(data)\n",
    "data = Transformer(binar_norm).fit_transform(data)\n",
    "data = Transformer(degrees, collect=['degrees']).fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_autism возвращает словарь с полями 'X', 'dist' и 'y'. А каждая функция в пакете binar_norm, degrees и т.д. принимает словарь с этими полями и изменяет эти поля, либо дополняет новые. Например, в случае degrees эта функция добавляет поле 'degrees'. Класс Transformer просто применяет к данным функцию аналогично FunctionTransformer в sklearn, но также он может вернуть тюпл X, y, где X --- выбранное в collect ( сейчас это ['degrees'] (обязательно лист)) поле, y --- значения по полю 'y'. Значение по полю 'y' выбирать не надо, это делается автоматом, также обязательно нужно по этому полю 'y' хранить таргет когда пишете функцию импорта данных, т.е. как в get_autism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим пайплайн. Пусть это будет селектор, скейлер и классификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [('selector', VarianceThreshold()), ('scaler', MinMaxScaler()), ('classifier', LogisticRegression())] \n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим параметры грид серча и сделаем грид серч. Параметры задаются в виде названиеШага\\_\\_параметрОбъекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('selector', VarianceThreshold(threshold=0.0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = dict(classifier__penalty=['l1', 'l2'])\n",
    "scoring = 'roc_auc'\n",
    "grid_clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, n_jobs=-1, cv=grid_cv)\n",
    "grid_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем классификатор с лучшими параметрами, зададим пайплайн с новым классификатором и оценим получившуюся модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47566666666666679, 0.17615050383124087)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps[-1] = steps[-1][0], grid_clf.best_estimator_\n",
    "pipeline = Pipeline(steps)\n",
    "scores = cross_val_score(pipeline, X, y, scoring=scoring, cv=eval_cv, n_jobs=-1)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все тоже самое делает класс Pipeliner. Сначала создается таблица."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Normalizer</th>\n",
       "      <th>Featurizer</th>\n",
       "      <th>Selector</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Normalizer Featurizer       Selector  Scaler Classifier\n",
       "0  UCLAsource      binar    degrees  var_threshold  minmax         LR"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('UCLAsource', Transformer(get_autism))]\n",
    "normalizer = [('binar', Transformer(binar_norm))]\n",
    "featurizer = [('degrees', Transformer(degrees, collect=['degrees']))]\n",
    "selector = [('var_threshold', VarianceThreshold())]\n",
    "scaler = [('minmax', MinMaxScaler())]\n",
    "classifier = [('LR', LogisticRegression())]\n",
    "\n",
    "steps = [('Data', data),\n",
    "         ('Normalizer', normalizer),\n",
    "         ('Featurizer', featurizer),\n",
    "         ('Selector', selector),\n",
    "         ('Scaler', scaler),\n",
    "         ('Classifier', classifier)]\n",
    "\n",
    "param_grid = dict(\n",
    "    LR=dict(\n",
    "        penalty=['l1', 'l2']\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe = Pipeliner(steps, eval_cv=eval_cv, grid_cv=grid_cv, param_grid=param_grid)\n",
    "pipe.plan_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем грид серч и валидирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                     Line: 1/1"
     ]
    }
   ],
   "source": [
    "results = pipe.get_results('Data/dti/', caching_steps=['Data', 'Normalizer', 'Featurizer'], scoring=['roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Normalizer</th>\n",
       "      <th>Featurizer</th>\n",
       "      <th>Selector</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>grid_roc_auc_mean</th>\n",
       "      <th>grid_roc_auc_std</th>\n",
       "      <th>grid_roc_auc_best_params</th>\n",
       "      <th>eval_roc_auc_mean</th>\n",
       "      <th>eval_roc_auc_std</th>\n",
       "      <th>eval_roc_auc_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.471099</td>\n",
       "      <td>0.171803</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "      <td>0.475667</td>\n",
       "      <td>0.176151</td>\n",
       "      <td>[ 0.46666667  0.6         0.24        0.9     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Normalizer Featurizer       Selector  Scaler Classifier  \\\n",
       "0  UCLAsource      binar    degrees  var_threshold  minmax         LR   \n",
       "\n",
       "  grid_roc_auc_mean grid_roc_auc_std grid_roc_auc_best_params  \\\n",
       "0          0.471099         0.171803        {'penalty': 'l2'}   \n",
       "\n",
       "  eval_roc_auc_mean eval_roc_auc_std  \\\n",
       "0          0.475667         0.176151   \n",
       "\n",
       "                                 eval_roc_auc_scores  \n",
       "0  [ 0.46666667  0.6         0.24        0.9     ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И если мы хотим попробовать много нормировок, признаков, классификаторов и т.д. То можно сделать следующее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем колонки и забаненные комбинации. Строк с одновременно встречающимися ключами в итоговой таблице не будет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [('UCLAsource', Transformer(get_autism))]\n",
    "\n",
    "weighters = [('origW', Transformer(orig)),\n",
    "             ('binar', Transformer(binar_norm)),\n",
    "             ('wbysqdist', Transformer(wbysqdist))]\n",
    "\n",
    "normalizers = [('origN', Transformer(orig)),\n",
    "               ('spectral', Transformer(spectral_norm))]\n",
    "\n",
    "featurizers = [('origF', Transformer(orig, collect=['X'])),\n",
    "               ('degrees', Transformer(degrees, collect=['degrees']))]\n",
    "\n",
    "selectors = [('var_threshold', VarianceThreshold())]\n",
    "\n",
    "scalers = [('minmax', MinMaxScaler()),\n",
    "           ('origS', FunctionTransformer(orig))]\n",
    "\n",
    "classifiers = [('LR', LogisticRegression()),\n",
    "               ('RF', RandomForestClassifier()),\n",
    "               ('SVC', SVC()),\n",
    "               ('XGB', XGBClassifier(nthread=1)),\n",
    "               ('SGD', SGDClassifier())]\n",
    "\n",
    "steps = [('Data', data),\n",
    "         ('Weighters', weighters),\n",
    "         ('Normalizers', normalizers),\n",
    "         ('Featurizers', featurizers),\n",
    "         ('Selectors', selectors),\n",
    "         ('Scalers', scalers),\n",
    "         ('Classifiers', classifiers)]\n",
    "\n",
    "banned_combos = [('UCLAsource', 'origN'),\n",
    "                 ('UCLAsource', 'origF'),\n",
    "                 ('UCLAbaseline', 'degrees'),\n",
    "                 ('UCLAbaseline', 'binar'),\n",
    "                 ('UCLAbaseline', 'wbysqdist'),\n",
    "                 ('UCLAbaseline', 'spectral'),\n",
    "                 ('LR', 'origS'),\n",
    "                 ('SVC', 'origS'),\n",
    "                 ('SGD', 'origS'),\n",
    "                 ('RF', 'minmax'),\n",
    "                 ('XGB', 'minmax')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем параметры грид серча и смотрим, какая таблица получилась. Если хотите вопроизвести результаты из статьи PRNI, то все параметры нужно раскомментировать, однако это считается ДОЛГО."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Weighters</th>\n",
       "      <th>Normalizers</th>\n",
       "      <th>Featurizers</th>\n",
       "      <th>Selectors</th>\n",
       "      <th>Scalers</th>\n",
       "      <th>Classifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>origS</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>origS</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>origS</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>binar</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>origS</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>wbysqdist</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>wbysqdist</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>wbysqdist</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>wbysqdist</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>origS</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>wbysqdist</td>\n",
       "      <td>spectral</td>\n",
       "      <td>degrees</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>origS</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Data  Weighters Normalizers Featurizers      Selectors Scalers  \\\n",
       "0   UCLAsource      origW    spectral     degrees  var_threshold  minmax   \n",
       "1   UCLAsource      origW    spectral     degrees  var_threshold  minmax   \n",
       "2   UCLAsource      origW    spectral     degrees  var_threshold  minmax   \n",
       "3   UCLAsource      origW    spectral     degrees  var_threshold   origS   \n",
       "4   UCLAsource      origW    spectral     degrees  var_threshold   origS   \n",
       "5   UCLAsource      binar    spectral     degrees  var_threshold  minmax   \n",
       "6   UCLAsource      binar    spectral     degrees  var_threshold  minmax   \n",
       "7   UCLAsource      binar    spectral     degrees  var_threshold  minmax   \n",
       "8   UCLAsource      binar    spectral     degrees  var_threshold   origS   \n",
       "9   UCLAsource      binar    spectral     degrees  var_threshold   origS   \n",
       "10  UCLAsource  wbysqdist    spectral     degrees  var_threshold  minmax   \n",
       "11  UCLAsource  wbysqdist    spectral     degrees  var_threshold  minmax   \n",
       "12  UCLAsource  wbysqdist    spectral     degrees  var_threshold  minmax   \n",
       "13  UCLAsource  wbysqdist    spectral     degrees  var_threshold   origS   \n",
       "14  UCLAsource  wbysqdist    spectral     degrees  var_threshold   origS   \n",
       "\n",
       "   Classifiers  \n",
       "0           LR  \n",
       "1          SVC  \n",
       "2          SGD  \n",
       "3           RF  \n",
       "4          XGB  \n",
       "5           LR  \n",
       "6          SVC  \n",
       "7          SGD  \n",
       "8           RF  \n",
       "9          XGB  \n",
       "10          LR  \n",
       "11         SVC  \n",
       "12         SGD  \n",
       "13          RF  \n",
       "14         XGB  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = dict(\n",
    "    LR=dict(\n",
    "#        C=[0.01, 0.05, 0.1] + [0.05*i for i in range(3, 21)],\n",
    "#        max_iter=[50, 100, 500],\n",
    "        penalty=['l1', 'l2']\n",
    "    ),\n",
    "    SGD=dict(\n",
    "#        alpha=[0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "#        l1_ratio=[0, 0.2, 0.4, 0.6, 0.8, 1],\n",
    "#        loss=['hinge', 'log', 'modified_huber'],\n",
    "        n_iter=[50, 100, 200],\n",
    "#        penalty=['elasticnet']\n",
    "    ),\n",
    "    SVC=dict(\n",
    "#        C=[0.0005, 0.001, 0.005, 0.01] + [i*0.05 for i in range(1,11)],\n",
    "#        degree=[2, 3, 4],\n",
    "#        kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        max_iter=[50, 100, 150],\n",
    "    ),\n",
    "    RF=dict(\n",
    "#        criterion=['entropy', 'gini'],\n",
    "#        max_depth=[3, 5, 7, 10, 20],\n",
    "#        max_features=['log2', 'sqrt'] + [0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 1.0],\n",
    "        n_estimators=[10, 50, 100, 200, 500]\n",
    "    ),\n",
    "    XGB=dict(\n",
    "#        colsample_bytree=[0.05*i for i in range(1,21)],\n",
    "#        learning_rate=[0.01*i for i in range(1,6)] + [0.05*i for i in range(2,11)],\n",
    "#        max_depth=[i for i in range(1,12)],\n",
    "#        n_estimators=[10, 50, 100, 200, 500],\n",
    "#        nthread=[1],\n",
    "#        reg_alpha=[0, 1],\n",
    "#        reg_lambda=[0, 1],\n",
    "        subsample=[0.5, 0.7, 1]\n",
    "    )\n",
    ")\n",
    "pipe = Pipeliner(steps, eval_cv=eval_cv, grid_cv=grid_cv, param_grid=param_grid, banned_combos=banned_combos)\n",
    "pipe.plan_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 1/15\n",
      "Line: 2/15\n",
      "Line: 3/15\n",
      "Line: 4/15\n",
      "Line: 5/15\n",
      "Line: 6/15\n",
      "Line: 7/15\n",
      "Line: 8/15\n",
      "Line: 9/15\n",
      "Line: 10/15\n",
      "Line: 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsdfyd/.venv/venv34/lib/python3.5/site-packages/reskit/norms.py:133: RuntimeWarning: invalid value encountered in true_divide\n",
      "  weighted_X = X / dist ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 12/15\n",
      "Line: 13/15\n"
     ]
    }
   ],
   "source": [
    "results = pipe.get_results('Data/dti/', caching_steps=['Data', 'Weighters', 'Normalizers', 'Featurizers'], scoring=['roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
